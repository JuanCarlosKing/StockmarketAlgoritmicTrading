{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":10736,"status":"ok","timestamp":1706995848869,"user":{"displayName":"Act Mirror","userId":"15615789685247570011"},"user_tz":-60},"id":"8kko9gJEsXLr"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import pandas as pd\n","from pandas import json_normalize\n","from pandas.tseries.offsets import DateOffset\n","import numpy as np\n","import bisect\n","import math\n","import statistics\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pandas import read_csv\n","from itertools import combinations\n","import urllib.request, json\n","import csv\n","from pandas import json_normalize\n","import math\n","import scipy.stats as sp\n","# The first thing we want to do is import the Pandas library and set the filepath to our data file\n","import warnings\n","from pandas.tseries.offsets import DateOffset\n","import warnings\n","import requests\n","import datetime\n","import seaborn as sns\n","import time\n","import statistics\n","import matplotlib.pyplot as plt\n","import urllib.request, json\n","\n","import seaborn as sns\n","from scipy.signal import argrelextrema\n","from collections import deque\n","from itertools import groupby, chain\n","from collections import OrderedDict\n","import os\n","import datetime as dt\n","from datetime import datetime, date, timedelta\n","import warnings\n","import re\n","\n","\n","import sklearn\n","from sklearn.metrics import roc_auc_score\n","from sklearn import metrics\n","from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn import metrics\n","from sklearn.metrics import precision_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix\n","from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, RandomForestRegressor\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import roc_auc_score, roc_curve\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","\n","import matplotlib.pyplot as plt\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, LSTM\n","\n","from itertools import groupby, chain\n","from collections import OrderedDict\n","\n","\n","def get_data_yfinance(asset, interval='1d', start='2011-01-01'):\n","    date_column = 'Date'\n","    list_of_days = ['1d', '5d', '1wk', '1mo', '3mo']\n","    list_of_minutes = ['1m', '2m', '5m', '15m', '30m', '60m', '90m']\n","    df = yf.download(asset, start=start, threads= False, interval=interval)\n","    df = df.reset_index()\n","    if interval== '1h':\n","        df = df.rename(columns={'index':'Date'})\n","    elif interval in list_of_minutes:\n","        df = df.rename(columns={'Datetime':'Date'})\n","        #df[date_column] = df[date_column].str[:]\n","\n","    # Now that we have loaded our data into the dataframe, we can preview it using the print \u0026 .head() function\n","\n","    df[date_column] = pd.to_datetime(df[date_column])\n","    df = df.sort_values([date_column], ascending=[True]).reset_index(drop=True).sort_values([date_column], ascending=[False])\n","    df = df.rename(columns={'Date':'date','Open':'open', 'High':'high', 'Low': 'low','Close':'close', 'Volume':'tradecount'})\n","    df['asset'] = asset\n","    if verificar_valores_sabados_domingos(df)==False:\n","        df = actualizar_dataframe_para_findesemana(df)\n","    return(df)\n","\n","def get_prices_indicators_data(ticker, date_column='date'):\n","    df = get_data_yfinance(ticker)[['date', 'open', 'high', 'low', 'close']]\n","\n","    df_unsorted = df.sort_values([date_column], ascending=[False]).reset_index(drop=True).sort_values([date_column], ascending=[True])\n","    df_direction = target(df_unsorted).direction()\n","    df_direction['target'] = df_direction['direction'].apply(lambda x: 1 if x\u003e 0 else 0)\n","    df_total = pd.merge(df,df_direction[['date', 'target']], on='date', how='inner')\n","    df_total['symbol']=ticker\n","    # 2 obtener indicadores y osciladores\n","    #INDICATORS\n","    df_squeez = squeez_momentum_indicator(df)\n","    df_squeez = df_squeez.rename(columns={'value':'value_squeez'})\n","    df_squeez = df_squeez.dropna()\n","    df_squeez = df_squeez.replace({True: 1, False: 0})\n","    list_of_squeez_indicator = get_list_of_fields(df_squeez)\n","    df_ind_osc, dictionary_of_indicators_oscilators = indicators_oscilators(df_unsorted)\n","    df_ind_osc =df_ind_osc.dropna()\n","    dictionary_of_indicators_oscilators['squeez'] = list_of_squeez_indicator\n","    df_ind_osc_target = pd.merge(df_total, df_ind_osc, on='date', how='inner')\n","    return df_ind_osc_target\n","\n","\n","class target:\n","    def __init__(self,df,column_date='date'):\n","        self.df = df\n","        self.column_date = column_date\n","\n","    def max_min(self, initial_days=0, days=10, column_target='high', method=max):\n","        df = self.df\n","        column_date = self.column_date\n","\n","        df_target = df[[column_date,column_target]].copy()\n","        df_target_original = df_target.copy()\n","        list_of_targets=[column_date]\n","        #in loop add one column per day\n","        for i in range(initial_days,days+1):\n","            df_target_temp = df_target_original.copy()\n","            df_target_temp[column_date+str(i)] = df_target_temp[column_date] + pd.Timedelta(days=-i)\n","            df_target_temp = df_target_temp[[column_date+str(i),column_target]].rename(columns={column_target:column_target+str(i)})\n","            df_target=pd.merge(df_target,df_target_temp, how='left',left_on=column_date, right_on=column_date+str(i))\n","            df_target = df_target.dropna()\n","            list_of_targets.append(column_target+str(i))\n","\n","        df_target = df_target[list_of_targets]\n","        if method==max:\n","            df_target['target'] = df_target[list_of_targets].max(axis=1)\n","        if method==min:\n","            df_target['target'] = df_target[list_of_targets].min(axis=1)\n","        df_target=df_target[[column_date, 'target']]\n","        df_target = df_target.sort_values([column_date], ascending=[True]).reset_index(drop=True).sort_values([column_date], ascending=[False])\n","\n","        return(df_target)\n","\n","    def close(self, days=10,column_target='close'):\n","        df = self.df\n","        column_date = self.column_date\n","        df_target = df[[column_date,column_target]].copy()\n","        df_target[column_date+ \"_-_\" +str(days)] = df_target[column_date] + pd.Timedelta(days=-days)\n","        df_target = df_target.drop(columns=column_date, axis=1)\n","        dictionary={column_date+ \"_-_\" +str(days):column_date, column_target:'target'}\n","        df_target = df_target.rename(columns=dictionary)\n","        df_target = df_target[[column_date, 'target']]\n","        return (df_target)\n","\n","    def direction(self,days=10,column_target='close', column_high='high', column_low='low'):\n","        df = self.df\n","        column_date = self.column_date\n","        df_close = df[[column_date, 'close']].copy()\n","        df_target_max = target(df).max_min(method=max).rename(columns={'target':'max'})\n","        df_target_min = target(df).max_min(method=min, column_target='low').rename(columns={'target':'min'})\n","        df_target_close = target(df).close().rename(columns={'target':'target_close'})\n","        df_final = pd.merge(df_close, df_target_max, on=column_date, how='inner')\n","        df_final = pd.merge(df_final, df_target_min, on=column_date, how='inner')\n","        df_final = pd.merge(df_final, df_target_close, on=column_date, how='inner')\n","        #df_final['direction'] = df_final['close'] + (df_final['max'] - df_final['close']  + df_final['min'] - df_final['close'] + df_final['target_close'] - df_final['close'])/3\n","        df_final['direction'] = ((df_final['max']- df_final['close'])/2 + (df_final['min']- df_final['close'])/2 )/2 + (df_final['target_close'] - df_final['close'])/2\n","        #df_final = df_final[['date','direction']]\n","\n","        return df_final\n","def verificar_valores_sabados_domingos(df):\n","    # Asegúrate de que la columna \"date\" esté en formato de fecha\n","    df['date'] = pd.to_datetime(df['date'])\n","\n","    # Obtiene el día de la semana para cada fecha (0 para lunes, 1 para martes, etc.)\n","    df['dayofweek'] = df['date'].dt.dayofweek\n","\n","    # Filtra las filas que corresponden a sábados (5) o domingos (6)\n","    sabados_domingos = df[(df['dayofweek'] == 5) | (df['dayofweek'] == 6)]\n","\n","    # Si \"sabados_domingos\" está vacío, significa que no hay valores para sábados ni domingos en el DataFrame\n","    if sabados_domingos.empty:\n","        weekend_good = False\n","    else:\n","        weekend_good = True\n","    return weekend_good\n","\n","\n","def actualizar_dataframe_para_findesemana(df):\n","    # Asegúrate de que la columna \"date\" esté en formato de fecha\n","    df['date'] = pd.to_datetime(df['date'])\n","\n","    # Agrega una columna \"week_day\" que contenga el día de la semana (0 para lunes, 1 para martes, etc.)\n","    df['week_day'] = df['date'].dt.dayofweek\n","\n","    # Crea un nuevo DataFrame para almacenar los datos actualizados\n","    new_df = pd.DataFrame(columns=df.columns)\n","\n","    # Recorre el DataFrame original\n","    for index, row in df.iterrows():\n","        new_df = new_df.append(row)\n","\n","        # Si el día de la semana es viernes (4)\n","        if row['week_day'] == 4:\n","            # Agrega una fila para el sábado (5)\n","            new_row_sabado = row.copy()\n","            new_row_sabado['date'] = row['date'] + timedelta(days=1)\n","            new_row_sabado['week_day'] = 5\n","            new_df = new_df.append(new_row_sabado)\n","\n","            # Agrega una fila para el domingo (6)\n","            new_row_domingo = row.copy()\n","            new_row_domingo['date'] = row['date'] + timedelta(days=2)\n","            new_row_domingo['week_day'] = 6\n","            new_df = new_df.append(new_row_domingo)\n","\n","    # Restablece el índice del nuevo DataFrame\n","    new_df.reset_index(drop=True, inplace=True)\n","\n","    return new_df\n","\n","def add_derivates(df, feature, periods=1):\n","    df_new = df.copy()\n","    diff_1 = df[[feature]].diff(periods=periods).rename(columns={feature: feature +\"_diff1\"})\n","    df_new = pd.concat([df_new, diff_1], axis=1).dropna()\n","    del df\n","    return df_new\n","\n","def get_ribbons(df, sma_low, sma_high, list_of_features, date_column='date'):\n","    df_out=df.copy()\n","    list_of_columns = [date_column] + list_of_features\n","    df_out=df_out[list_of_columns]\n","    list_of_sma = []\n","    list_of_ribbon = []\n","    for i in list_of_features:\n","        sma_low_ = i + '_sma_' + str(sma_low)\n","        list_of_sma.append(sma_low_)\n","        df_out[sma_low_] = pd.Series.to_frame(ta.sma(df_out[i], length=sma_low))\n","        sma_high_ = i + '_sma_' + str(sma_high)\n","        list_of_sma.append(sma_high_)\n","        df_out[sma_high_] = pd.Series.to_frame(ta.sma(df_out[i], length=sma_high))\n","        ribbon_ = 'ribbon_' + i + '_' +  str(sma_low) + '_' + str(sma_high)\n","        list_of_ribbon.append(ribbon_)\n","        df_out[ribbon_] = (df_out[sma_low_] - df_out[sma_high_])\n","    df_out = df_out.dropna()\n","    return df_out\n","\n","\n","#########################################################\n","\n","def grouper(_lst, interval=0.04):\n","    z = zip(_lst,_lst[1:])\n","    return [OrderedDict.fromkeys(chain.from_iterable(g)).keys() for k,g in groupby(z,key=lambda x:x[1]-x[0]\u003cinterval) if k]\n","\n","def supports_and_resistences(df, interval=0.04, column_close='close', column_high='high', column_low='low', \\\n","                             date_column='date', look_back=200, window=20):\n","    df_new = df.copy()\n","    iters = int(look_back/window)\n","    data_df = {date_column: '', 'second_last_support':[''], 'last_support':[''], 'last_resistence':[''], 'second_last_resistence': ['']}\n","    df_supports_and_resistances = pd.DataFrame(data=data_df)\n","    df_supports_and_resistances = df_supports_and_resistances[0:0]\n","    #count=0\n","\n","    # Take resistances only until the lookback normally 200\n","    for ii in range(0,len(df)-look_back):\n","        value = df[column_close].iloc[ii]\n","        date = df[date_column].iloc[ii]\n","        df_temp=df_new[ii:look_back+ii]\n","        list_of_points=[]\n","\n","        for it in range(0,iters):\n","            df_small = df_temp[window*it:window*it+window]\n","            max_high = df_small[column_high].max()\n","            max_close = df_small[column_close].max()\n","            min_low = df_small[column_low].min()\n","            min_close = df_small[column_close].min()\n","            list_of_new_points = [max_high, max_close, min_low, min_close]\n","            list_of_points += list_of_new_points\n","            list_of_points = sorted(list_of_points)\n","        new_list = grouper(list_of_points, value * interval)\n","        list_of_final_points=[]\n","\n","        for j in range(len(new_list)):\n","            list_of_final_points.append(statistics.mean(new_list[j]))\n","        list_of_close_points=calculate_diferentials_resistances(list_of_final_points, value)\n","        #create dataframe\n","        d = {date_column: [date], 'second_last_support':[list_of_close_points[0]], 'last_support': [list_of_close_points[1]], \\\n","             'last_resistence': [list_of_close_points[2]], 'second_last_resistence': [list_of_close_points[3]]}\n","        df_temporal = pd.DataFrame(data=d)\n","        df_supports_and_resistances = pd.concat([df_supports_and_resistances, df_temporal], axis=0)\n","    return(df_supports_and_resistances)\n","\n","def get_list_of_fields(df, date_column='date', *args):\n","    list_of_fields = list(df.columns)\n","    if date_column in list_of_fields:\n","        list_of_fields.remove(date_column)\n","    if len(args)\u003e0:\n","        for new_column_to_delete in args:\n","            if new_column_to_delete in list_of_fields:\n","                list_of_fields.remove(new_column_to_delete)\n","    return list_of_fields\n","\n","def supports_and_resistences_more_days(df,days_back, window_=20):\n","    df_sr = supports_and_resistences(df, look_back=days_back, window=window_)\n","    df_sr = df_sr.rename(columns={'second_last_support':'second_last_support_'+ str(days_back),\n","                          'last_support': 'last_support_'+ str(days_back),\n","                          'last_resistence': 'last_resistence_'+ str(days_back),\n","                          'second_last_resistence':'second_last_resistence_'+ str(days_back)})\n","    return df_sr\n","\n","def get_fibonacci(df,days=200, column_close='close', column_high='high', column_low='low', date_column='date', list_of_points=[0, 0.236, 0.382, 0.5, 0.618, 0.786, 1, 1.278, 1.414]):\n","    list_of_points=list_of_points\n","    def calculate_fibonacci_levels(price_max, price_min, list_of_points):\n","        diff = price_max-price_min\n","        fibo_levels = [(price_min + i *diff) for i in list_of_points]\n","        return(fibo_levels)\n","\n","    data_df = {date_column: '', 'second_fibo_last_support':[''], 'last_fibo_support':[''], 'last_fibo_resistence':[''], 'second_last_fibo_resistence': ['']}\n","    df_supports_and_resistances = pd.DataFrame(data=data_df)\n","    df_supports_and_resistances = df_supports_and_resistances[0:0]\n","\n","    for ii in range(0,len(df)-days):\n","        df_temp=df[ii:days+ii].copy()\n","        value = df[column_close].iloc[ii]\n","        date = df[date_column].iloc[ii]\n","        price_max = df_temp[column_high].max()\n","        price_min = df_temp[column_low].min()\n","        list_of_final_points=calculate_fibonacci_levels(price_max, price_min, list_of_points)\n","        list_of_close_points=calculate_diferentials_resistances(list_of_final_points, value)\n","        #create dataframe\n","        d = {'date': [date], 'second_fibo_last_support':[list_of_close_points[0]], 'last_fibo_support': [list_of_close_points[1]], 'last_fibo_resistence': [list_of_close_points[2]], 'second_last_fibo_resistence': [list_of_close_points[3]]}\n","        df_temporal = pd.DataFrame(data=d)\n","        df_supports_and_resistances = pd.concat([df_supports_and_resistances, df_temporal], axis=0)\n","\n","    for ii in range(len(df)-days,len(df)):\n","        df_temp=df[ii:len(df)].copy()\n","        value = df[column_close].iloc[ii]\n","        date = df[date_column].iloc[ii]\n","        price_max = df_temp[column_high].max()\n","        price_min = df_temp[column_low].min()\n","        list_of_final_points=calculate_fibonacci_levels(price_max, price_min, list_of_points)\n","        list_of_close_points=calculate_diferentials_resistances(list_of_final_points, value)\n","        #create dataframe\n","        d = {date_column: [date], 'second_fibo_last_support':[list_of_close_points[0]], 'last_fibo_support': [list_of_close_points[1]], 'last_fibo_resistence': [list_of_close_points[2]], 'second_last_fibo_resistence': [list_of_close_points[3]]}\n","        df_temporal = pd.DataFrame(data=d)\n","        df_supports_and_resistances = pd.concat([df_supports_and_resistances, df_temporal], axis=0)\n","\n","\n","    #return(list(df_supports_and_resistances.columns),df_supports_and_resistances)\n","    return(df_supports_and_resistances)\n","\n","def get_fibonacci_levels(df):\n","    df_close = df[['date', 'close']]\n","    df_fibo = get_fibonacci(df)\n","    df_fibo_to_clean = pd.merge(df_close, df_fibo, on='date', how='left').replace('NOT FOUND', -1)\n","    df_fibo_filled = fill_support_not_found_values_in_sr(df_fibo_to_clean, 'last_fibo_support')\n","    df_fibo_filled = fill_support_not_found_values_in_sr(df_fibo_filled, 'second_fibo_last_support', measure_column='last_fibo_support')\n","    minimum_value = df_fibo_filled.close.min()\n","    df_fibo_filled = df_fibo_filled.replace({'last_fibo_support': {-1: minimum_value},\\\n","                           'second_fibo_last_support': {-1: minimum_value}})\n","    df_fibo_filled = df_fibo_filled.drop(\"close\", axis='columns')\n","\n","    return(df_fibo_filled)\n","\n","def obtain_list_of_volumen_areas(df, column_close='close', volume_column='tradecount', aggregation=1000):\n","    agg = int(df[column_close].max()/aggregation)\n","    if agg==0:\n","        agg=1\n","    data_vp = df[[column_close, volume_column]].copy()\n","    data_vp[column_close] = data_vp[column_close].apply(lambda x: int(agg * round(float(x)/agg)))\n","    data_vp = data_vp.set_index(column_close)\n","    data_vp = data_vp.groupby([column_close]).sum()\n","    df_areas = data_vp.sort_values(by=volume_column, ascending=False).reset_index()\n","    # take only the quartil\n","    mean_of_df = df_areas[['tradecount']].mean()\n","    max_of_df = df_areas[['tradecount']].max()\n","    std_of_df = df_areas[['tradecount']].std()\n","    threshold = mean_of_df - std_of_df\n","    df_temp_areas = df_areas[df_areas['tradecount']\u003ethreshold[0]]\n","    #convert_to_list\n","    list_of_areas = df_temp_areas[column_close].tolist()\n","    list_of_areas=sorted(list_of_areas)\n","    new_list = grouper(list_of_areas, interval=int(df[column_close].max()/(len(df)/5)))\n","    list_of_final_points = [statistics.mean(new_list[j]) for j in range(len(new_list))]\n","    return(list_of_final_points)\n","\n","def get_area_volume(df, column_close='close', volume_column='tradecount', date_column='date'):\n","    data_df = {date_column: '', 'second_area_last_support':[''], 'last_area_support':[''], 'last_area_resistence':[''], 'second_area_resistence': ['']}\n","    df_supports_and_resistances = pd.DataFrame(data=data_df)\n","    df_supports_and_resistances = df_supports_and_resistances[0:0]\n","    for i in range(0,len(df)):\n","        df_temp = df[i:len(df)]\n","        value = df[column_close].iloc[i]\n","        date = df[date_column].iloc[i]\n","        list_of_final_points = obtain_list_of_volumen_areas(df_temp)\n","        list_of_close_points=calculate_diferentials_resistances(list_of_final_points, value)\n","        #create dataframe\n","        d = {date_column: [date], 'second_area_last_support':[list_of_close_points[0]], 'last_area_support': [list_of_close_points[1]], 'last_area_resistence': [list_of_close_points[2]], 'second_area_resistence': [list_of_close_points[3]]}\n","        df_temporal = pd.DataFrame(data=d)\n","        df_supports_and_resistances = pd.concat([df_supports_and_resistances, df_temporal], axis=0)\n","    return(df_supports_and_resistances)\n","\n","def get_volume_areas(df, date_column='date', close_column='close'):\n","    df_close = df[[date_column, close_column]]\n","    df_areas = get_area_volume(df)\n","    df_areas_and_close = pd.merge(df_close, df_areas, on='date', how='inner').replace('NOT FOUND', -1).replace('UNKNOWN', -1)\n","    df_filled = fill_support_not_found_values_in_sr(df_areas_and_close, 'last_area_support')\n","    df_filled = fill_support_not_found_values_in_sr(df_filled, 'second_area_last_support', measure_column='last_area_support')\n","    df_filled = fill_resistence_not_found_values_in_sr(df_filled, 'last_area_resistence')\n","    df_filled = fill_resistence_not_found_values_in_sr(df_filled, 'second_area_resistence', measure_column='last_area_resistence')\n","    df_out = df_filled.copy()\n","    for i in range(len(df_out)):\n","        df_it_out = df_out[i:len(df_out)]\n","        maximum_value = df_it_out.close.max()\n","        minimum_value = df_it_out.close.min()\n","        if df_out[['last_area_support']].iloc[i][0]==-1:\n","            df_out.at[i, 'last_area_support'] = minimum_value\n","        if df_out[['second_area_last_support']].iloc[i][0]==-1:\n","            df_out.at[i, 'second_area_last_support'] = minimum_value\n","        if df_out[['last_area_resistence']].iloc[i][0]==-1:\n","            df_out.at[i, 'last_area_resistence'] = maximum_value\n","        if df_out[['second_area_resistence']].iloc[i][0]==-1:\n","            df_out.at[i, 'second_area_resistence'] = maximum_value\n","    df_out =df_out.drop(columns=close_column, axis=1)\n","    return df_out\n","\n","def pivot_cpr(df, high='high', close='close', low='low', agg='M'):\n","    df = df.copy()\n","    df['Pivot'+agg] = (df[high] + df[low] + df[close])/3\n","    df['BC'+agg] = (df[high] + df[low])/2\n","    df['TC'+agg] = (df['Pivot'+agg] - df['BC'+agg]) + df['Pivot'+agg]\n","    df['R1'+agg] = 2*df['Pivot'+agg] - df[low]\n","    df['R2'+agg] = df['Pivot'+agg] + (df[high] - df[low])\n","    df['R3'+agg] = df['R1'+agg] + (df[high] - df[low])\n","    #df['R4'+agg] = df['R3'+agg] + (df['R2'+agg] - df['R1'+agg])\n","    df['S1'+agg] = 2*df['Pivot'+agg] - df[high]\n","    df['S2'+agg] = df['Pivot'+agg] - (df[high] - df[low])\n","    df['S3'+agg] = df['S1'+agg] - (df[high] - df[low])\n","    #df['S4'+agg] = df['S3'+agg] - (df['S1'+agg] - df['S2'+agg])\n","    return(df)\n","\n","def index_timeframe(df, aggregation='W', date_column='date'):\n","    df2 = df.copy()\n","    df2[date_column] = pd.to_datetime(df[date_column], format='%Y-%m-%d')\n","    df2.set_index(date_column, inplace=True)\n","    df2.sort_index(inplace=True)\n","    tradecount = df2[['tradecount']].resample(aggregation).mean()\n","    high = df2[['high']].resample(aggregation).max()\n","    low = df2[['low']].resample(aggregation).min()\n","    open_ = df2[['open']].resample(aggregation).first()\n","    close = df2[['close']].resample(aggregation).last()\n","    #Volume_asset = df2[['Volume asset']].resample(aggregation).mean()\n","    #Volume_pair = df2[['Volume pair']].resample(aggregation).mean()\n","\n","    list_of_fields = ['date_'+str(aggregation),'open_'+str(aggregation), 'high_'+str(aggregation), 'low_'+str(aggregation), 'close_'+str(aggregation), 'Volume_BTC_'+str(aggregation), 'Volume_USDT_'+str(aggregation), 'tradecount_'+str(aggregation), 'date_join_'+str(aggregation)]\n","    list_of_df = [high, low, close, tradecount]\n","\n","    #list_of_columns = [date_column, 'open', 'high', 'low', 'close', 'Volume asset', 'Volume pair', 'tradecount', 'date_join']\n","\n","    df_final = open_\n","    for i in list_of_df:\n","        df_final = pd.concat([df_final,i], axis=1)\n","    df_final = df_final.reset_index()\n","    df_final[date_column] = pd.to_datetime(df_final[date_column],format='%Y-%m-%d')\n","    df_final = df_final.sort_values(['date'], ascending=[True]).reset_index(drop=True).sort_values(['date'], ascending=[False])\n","\n","\n","    if aggregation=='W':\n","        alfa=7\n","    elif aggregation=='M':\n","        alfa=28\n","    df_final['date_join'] = pd.to_datetime(df_final.date) + pd.to_timedelta(+alfa, unit=\"D\")\n","\n","    #df_final = indicators_oscilators(df_final)\n","\n","\n","    list_of_fields = [(str(df_final.columns[i])+ \"_\"+aggregation) for i in range(len(df_final.columns))]\n","    list_of_columns = df_final.columns\n","    dictionary_columns = zip(list_of_columns, list_of_fields)\n","    dictionary = dict(dictionary_columns)\n","    df_final.rename(dictionary, axis=1, inplace=True)\n","    df_final = df_final.tail(len(df_final)-1)\n","\n","    #df_final = df_final.dropna()\n","\n","    return(df_final)\n","\n","def get_monthly_cpr(df, column_date_day ='date',column_close='close', volume_column='tradecount', column_open='open',\n","                      column_high='high', column_low='low'):\n","    aggregation='M'\n","    df_d = df.copy()\n","    df_d['year'] = df_d[column_date_day].dt.year\n","    df_d['week_day']=df_d[column_date_day].dt.dayofweek\n","    df_d['week_num']=df_d[column_date_day].dt.isocalendar().week\n","    df_d['month_num'] = df_d[column_date_day].dt.month\n","\n","    #monthly variables\n","    df_m = index_timeframe(df, aggregation=aggregation)\n","    date_m = column_date_day + '_' + aggregation\n","    date_join = column_date_day + '_join_' + aggregation\n","    open_m = column_open + '_' + aggregation\n","    close_m = column_close + '_' + aggregation\n","    high_m = column_high + '_' + aggregation\n","    low_m = column_low + '_' + aggregation\n","    #variables to merge\n","    df_m = df_m[[date_m, date_join, open_m, high_m , low_m, close_m]]\n","    df_m_pivot = pivot_cpr(df_m, high=high_m, close=close_m, low=low_m)\n","    df_m_pivot['year'] = df_m_pivot[date_join].dt.year\n","    df_m_pivot['month_num'] = df_m_pivot[date_join].dt.month\n","    df_m_pivot = df_m_pivot.drop(columns=[date_m, date_join], axis=1)\n","    #merge\n","    df_cpr = pd.merge(df_d,df_m_pivot, how='left', on=['year','month_num'])\n","\n","    data_df = {column_date_day: '', 'second_cpr_last_support':[''], 'last_cpr_support':[''], 'last_cpr_resistence':[''], 'second_last_cpr_resistence': ['']}\n","    df_supports_and_resistances = pd.DataFrame(data=data_df)\n","    df_supports_and_resistances = df_supports_and_resistances[0:0]\n","\n","    for i in range(len(df_cpr)):\n","        list_of_points = df_cpr[['PivotM', 'BCM', 'TCM', 'R1M', 'R2M', 'R3M', 'S1M', 'S2M', 'S3M']].iloc[i]\n","        list_of_points = sorted(list_of_points)\n","        value=df_cpr[column_close].iloc[i]\n","        date=df_cpr[column_date_day].iloc[i]\n","        list_of_close_points=calculate_diferentials_resistances(list_of_points, value)\n","        #create dataframe\n","        d = {column_date_day: [date], 'second_cpr_last_support':[list_of_close_points[0]], 'last_cpr_support': [list_of_close_points[1]], 'last_cpr_resistence': [list_of_close_points[2]], 'second_last_cpr_resistence': [list_of_close_points[3]]}\n","        df_temporal = pd.DataFrame(data=d)\n","        df_supports_and_resistances = pd.concat([df_supports_and_resistances, df_temporal], axis=0)\n","\n","    return(df_supports_and_resistances)\n","\n","def get_monthly_cpr_points(df, date_column='date', close_column='close'):\n","    df_close = df[[date_column, close_column]].copy()\n","    df_cpr = get_monthly_cpr(df)\n","    df_cpr_and_close = pd.merge(df_close, df_cpr, on='date', how='inner').replace('NOT FOUND', -1).replace('UNKNOWN', -1)\n","    df_filled = fill_support_not_found_values_in_sr(df_cpr_and_close, 'last_cpr_support')\n","    df_filled = fill_support_not_found_values_in_sr(df_filled, 'second_cpr_last_support', measure_column='last_cpr_support')\n","    df_filled = fill_resistence_not_found_values_in_sr(df_filled, 'last_cpr_resistence')\n","    df_filled = fill_resistence_not_found_values_in_sr(df_filled, 'second_last_cpr_resistence', measure_column='last_cpr_resistence')\n","    df_out = df_filled.dropna().copy()\n","    for i in range(len(df_out)):\n","        df_it_out = df_out[i:len(df_out)]\n","        maximum_value = df_it_out.close.max()\n","        minimum_value = df_it_out.close.min()\n","        if df_out[['last_cpr_support']].iloc[i][0]==-1:\n","            df_out.at[i, 'last_cpr_support'] = minimum_value\n","        if df_out[['second_cpr_last_support']].iloc[i][0]==-1:\n","            df_out.at[i, 'second_cpr_last_support'] = minimum_value\n","        if df_out[['last_cpr_resistence']].iloc[i][0]==-1:\n","            df_out.at[i, 'last_cpr_resistence'] = maximum_value\n","        if df_out[['second_last_cpr_resistence']].iloc[i][0]==-1:\n","            df_out.at[i, 'second_last_cpr_resistence'] = maximum_value\n","    df_out =df_out.drop(columns=close_column, axis=1)\n","    return(df_out)\n","\n","def get_fear_and_greed():\n","    url = 'https://api.alternative.me/fng/?limit=0\u0026format=json'\n","    with urllib.request.urlopen(url) as url:\n","        data = json.loads(url.read().decode())\n","    df = json_normalize(data['data'])\n","    df['timestamp'] = pd.to_datetime(df['timestamp'],unit='s')\n","    df = df.drop(columns = ['time_until_update', 'value_classification'])\n","    df = df.rename(columns={'timestamp':'date'})\n","    df = df[['date', 'value']]\n","    return df\n","\n","\n","#######################################################################\n","\n","\n","def indicators_oscilators(df):\n","    dictionary_of_features = {}\n","    initial_columns=list(df.columns)\n","    initial_columns = initial_columns[1:]\n","    #df = df.sort_index(ascending=True)\n","    list_of_indicators = []\n","    # INDICATORS #  queda pendiente revisar el span\n","\n","    try:\n","        willr = pd.Series.to_frame(ta.willr(df['high'],df['low'],df['close']))\n","        list_of_indicators.append(willr)\n","        dictionary_of_features[\"willr\"] = get_list_of_fields(willr)\n","    except:\n","        print(\"william no da valores\")\n","    try:\n","        sma20 = pd.Series.to_frame(ta.sma(df[\"close\"], length=20))\n","        list_of_indicators.append(sma20)\n","        dictionary_of_features[\"sma20\"] = get_list_of_fields(sma20)\n","    except:\n","        print(\"sma20 no da valores\")\n","    try:\n","        ema20_ohlc4 = pd.Series.to_frame(df.ta.ema(close=df.ta.ohlc4(), length=20, suffix=\"OHLC4\"))\n","        list_of_indicators.append(ema20_ohlc4)\n","        dictionary_of_features[\"ema20\"] = get_list_of_fields(ema20_ohlc4)\n","    except:\n","        print(\"ema20_ohlc4 no da valores\")\n","    try:\n","        sma55 = pd.Series.to_frame(ta.sma(df[\"close\"], length=55))\n","        list_of_indicators.append(sma55)\n","        dictionary_of_features[\"sma55\"] = get_list_of_fields(sma55)\n","    except:\n","        print(\"sma55 no da valores\")\n","    try:\n","        ema55_ohlc4 = pd.Series.to_frame(df.ta.ema(close=df.ta.ohlc4(), length=55, suffix=\"OHLC4\"))\n","        list_of_indicators.append(ema55_ohlc4)\n","        dictionary_of_features[\"ema55\"] = get_list_of_fields(ema55_ohlc4)\n","    except:\n","        print(\"ema55_ohlc4 no da valores\")\n","\n","    try:\n","        ema200_ohlc4 = pd.Series.to_frame(df.ta.ema(close=df.ta.ohlc4(), length=200, suffix=\"OHLC4\"))\n","        list_of_indicators.append(ema200_ohlc4)\n","        dictionary_of_features[\"ema200\"] = get_list_of_fields(ema200_ohlc4)\n","    except:\n","        print(\"ema200_ohlc4 no da valores\")\n","\n","    try:\n","        ichimoku, span = ta.ichimoku(df[\"high\"], df[\"low\"], df[\"close\"]) #review span\n","        ichimoku = ichimoku.drop(columns=['ICS_26'])\n","        list_of_indicators.append(ichimoku)\n","        dictionary_of_features[\"ichimoku\"] = get_list_of_fields(ichimoku)\n","    except:\n","        print(\"ichimoku no da valores\")\n","    try:\n","        adx = ta.adx(df[\"high\"], df[\"low\"], df[\"close\"])\n","        list_of_indicators.append(adx)\n","        dictionary_of_features[\"adx\"] = get_list_of_fields(adx)\n","    except:\n","        print(\"adx no da valores\")\n","    try:\n","        bbands = ta.bbands(df[\"close\"])\n","        list_of_indicators.append(bbands)\n","        dictionary_of_features[\"bbands\"] = get_list_of_fields(bbands)\n","    except:\n","        print(\"bbands no da valores\")\n","\n","    # OSCILATORS #\n","    list_of_oscilators = []\n","\n","    try:\n","        atr = pd.Series.to_frame(ta.atr(df['high'],df['low'],df['close']))\n","        list_of_oscilators.append(atr)\n","        dictionary_of_features[\"atr\"] = get_list_of_fields(atr)\n","    except:\n","        print(\"momentum no da valores\")\n","    try:\n","        momentum = ta.squeeze(df[\"high\"], df[\"low\"], df[\"close\"])\n","        list_of_oscilators.append(momentum)\n","        dictionary_of_features[\"momentum \"] = get_list_of_fields(momentum )\n","    except:\n","        print(\"momentum no da valores\")\n","    try:\n","        rsi = pd.Series.to_frame(ta.rsi(df[\"close\"]))\n","        list_of_oscilators.append(rsi)\n","        dictionary_of_features[\"rsi\"] = get_list_of_fields(rsi)\n","    except:\n","        print(\"rsi no da valores\")\n","    try:\n","        rsi_6 = pd.Series.to_frame(ta.rsi(df[\"close\"], length=6))\n","        list_of_oscilators.append(rsi_6)\n","        dictionary_of_features[\"rsi_6\"] = get_list_of_fields(rsi_6)\n","    except:\n","        print(\"rsi 6 no da valores\")\n","    try:\n","        rsi_12 = pd.Series.to_frame(ta.rsi(df[\"close\"], length=12))\n","        list_of_oscilators.append(rsi_12)\n","        dictionary_of_features[\"rsi_12\"] = get_list_of_fields(rsi_12)\n","    except:\n","        print(\"rsi 12 no da valores\")\n","    try:\n","        rsi_24 = pd.Series.to_frame(ta.rsi(df[\"close\"], length=24))\n","        list_of_oscilators.append(rsi_24)\n","        dictionary_of_features[\"rsi_24\"] = get_list_of_fields(rsi_24)\n","    except:\n","        print(\"rsi 24  no da valores\")\n","    try:\n","        macd = ta.macd(df[\"close\"])\n","        list_of_oscilators.append(macd)\n","        dictionary_of_features[\"macd\"] = get_list_of_fields(macd)\n","    except:\n","        print(\"macd no da valores\")\n","    try:\n","        kdj = ta.kdj(df[\"high\"], df[\"low\"], df[\"close\"])\n","        list_of_oscilators.append(kdj)\n","        dictionary_of_features[\"kdj\"] = get_list_of_fields(kdj)\n","    except:\n","        print(\"kdj  no da valores\")\n","\n","    for indicator in list_of_indicators:\n","        df = pd.concat([df,indicator],axis=1)\n","    for oscilator in list_of_oscilators:\n","        df = pd.concat([df,oscilator],axis=1)\n","\n","    df = df.sort_index(ascending=False)\n","\n","    list_of_columns = df.columns\n","    for column in list_of_columns:\n","        if df[column].isnull().all() == True:\n","            df = df.drop(columns=column, axis=1)\n","    df = df.drop(columns=initial_columns, axis=1)\n","\n","    return(df, dictionary_of_features)\n","\n","\n","#########################################################\n","\n","\n","def get_rate_volume(filepath_head, filepath_tail, asset, pair):\n","    #filepath_head = \"https://www.cryptodatadownload.com/cdd/Binance_\"\n","    #filepath_tail = \"_d.csv\"\n","    #filepath = \"https://www.cryptodatadownload.com/cdd/Binance_BTCUSDT_d.csv\"\n","    # Now we want to create a dataframe and use Pandas' to_csv function to read in our file\n","    filepath2 = filepath_head + '%s%s' % (asset,pair) +  filepath_tail\n","    df = pd.read_csv(filepath2, skiprows=1)  # we use skiprows parameter because first row contains our web address\n","    # Now that we have loaded our data into the dataframe, we can preview it using the print \u0026 .head() function\n","    df['date'] = df['date'].str[:10]\n","    df['date'] = pd.to_datetime(df['date'],format='%Y-%m-%d')\n","    df = df.sort_values(['date'], ascending=[True]).reset_index(drop=True).sort_values(['date'], ascending=[False])\n","    vol_asset ='Volume %s' %(asset)\n","    vol_pair ='Volume %s' %(pair)\n","    df = df.rename(columns={vol_asset:'Volume asset', vol_pair:'Volume pair'})\n","    df = df[['date', 'Volume asset','Volume pair', 'close']]\n","    df['volume_rate'] = df['Volume pair'] / df['Volume asset']\n","    df['volume_rate_vs_close'] = (df['Volume pair'] / df['Volume asset'])/df['close']\n","\n","    return(df)\n","\n","#########################################################\n","\n","\n","def squeez_momentum_indicator(df, column_close='close', column_date='date', column_high='high', column_low='low',\n","                              length = 20, mult = 2, length_KC = 20, mult_KC = 1.5):\n","\n","    df = df.sort_values([column_date], ascending=[False]).reset_index(drop=True).sort_values([column_date], ascending=[True])\n","\n","    df = df.copy()\n","    df=df[[column_date, column_close, column_high, column_low]]\n","    # calculate BB\n","    m_avg = df[column_close].rolling(window=length).mean()\n","    m_std = df[column_close].rolling(window=length).std(ddof=0)\n","    df['upper_BB'] = m_avg + mult * m_std\n","    df['lower_BB'] = m_avg - mult * m_std\n","\n","    # calculate true range\n","    df['tr0'] = abs(df[column_high] - df[column_low])\n","    df['tr1'] = abs(df[column_high] - df[column_close].shift())\n","    df['tr2'] = abs(df[column_low] - df[column_close].shift())\n","    df['tr'] = df[['tr0', 'tr1', 'tr2']].max(axis=1)\n","\n","    # calculate KC\n","    range_ma = df['tr'].rolling(window=length_KC).mean()\n","    df['upper_KC'] = m_avg + range_ma * mult_KC\n","    df['lower_KC'] = m_avg - range_ma * mult_KC\n","\n","    # calculate bar value\n","    highest = df[column_high].rolling(window = length_KC).max()\n","    lowest = df[column_low].rolling(window = length_KC).min()\n","    m1 = (highest + lowest)/2\n","    df['value'] = (df[column_close] - (m1 + m_avg)/2)\n","    fit_y = np.array(range(0,length_KC))\n","    df['value'] = df['value'].rolling(window = length_KC).apply(\n","        lambda x: np.polyfit(fit_y, x, 1)[0] * (length_KC-1) + np.polyfit(fit_y, x, 1)[1], raw=True\n","    )\n","\n","    # check for 'squeeze'\n","    df['squeeze_on'] = (df['lower_BB'] \u003e df['lower_KC']) \u0026 (df['upper_BB'] \u003c df['upper_KC'])\n","    df['squeeze_off'] = (df['lower_BB'] \u003c df['lower_KC']) \u0026 (df['upper_BB'] \u003e df['upper_KC'])\n","\n","    #drop main features\n","\n","    df = df[[column_date, 'value', 'squeeze_on', 'squeeze_off', 'tr0','tr1','tr2','tr']]\n","\n","    return df\n","\n","####################################################\n","\n","\n","def squeez_momentum_indicator(df, column_close='close', column_date='date', column_high='high', column_low='low',\n","                              length = 20, mult = 2, length_KC = 20, mult_KC = 1.5):\n","\n","    df = df.sort_values([column_date], ascending=[False]).reset_index(drop=True).sort_values([column_date], ascending=[True])\n","\n","    df = df.copy()\n","    df=df[[column_date, column_close, column_high, column_low]]\n","    # calculate BB\n","    m_avg = df[column_close].rolling(window=length).mean()\n","    m_std = df[column_close].rolling(window=length).std(ddof=0)\n","    df['upper_BB'] = m_avg + mult * m_std\n","    df['lower_BB'] = m_avg - mult * m_std\n","\n","    # calculate true range\n","    df['tr0'] = abs(df[column_high] - df[column_low])\n","    df['tr1'] = abs(df[column_high] - df[column_close].shift())\n","    df['tr2'] = abs(df[column_low] - df[column_close].shift())\n","    df['tr'] = df[['tr0', 'tr1', 'tr2']].max(axis=1)\n","\n","    # calculate KC\n","    range_ma = df['tr'].rolling(window=length_KC).mean()\n","    df['upper_KC'] = m_avg + range_ma * mult_KC\n","    df['lower_KC'] = m_avg - range_ma * mult_KC\n","\n","    # calculate bar value\n","    highest = df[column_high].rolling(window = length_KC).max()\n","    lowest = df[column_low].rolling(window = length_KC).min()\n","    m1 = (highest + lowest)/2\n","    df['value'] = (df[column_close] - (m1 + m_avg)/2)\n","    fit_y = np.array(range(0,length_KC))\n","    df['value'] = df['value'].rolling(window = length_KC).apply(\n","        lambda x: np.polyfit(fit_y, x, 1)[0] * (length_KC-1) + np.polyfit(fit_y, x, 1)[1], raw=True\n","    )\n","\n","    # check for 'squeeze'\n","    df['squeeze_on'] = (df['lower_BB'] \u003e df['lower_KC']) \u0026 (df['upper_BB'] \u003c df['upper_KC'])\n","    df['squeeze_off'] = (df['lower_BB'] \u003c df['lower_KC']) \u0026 (df['upper_BB'] \u003e df['upper_KC'])\n","\n","    #drop main features\n","\n","    df = df[[column_date, 'value', 'squeeze_on', 'squeeze_off', 'tr0','tr1','tr2','tr']]\n","\n","    return df\n","\n","def calculate_diferentials_resistances(lista, valor_buscado):\n","    last_support = \"UNKNOWN\"\n","    second_last_support = \"UNKNOWN\"\n","    last_resistence= \"UNKNOWN\"\n","    second_last_resistence = \"UNKNOWN\"\n","    posicion = bisect.bisect_left(lista, valor_buscado)\n","    if len(lista) == 0:\n","        print(\"no hay valores encontrados en: \", valor_buscado, lista)\n","    elif len(lista) == 1:\n","        if posicion == 0:\n","            last_support = \"NOT FOUND\"\n","            second_last_support = \"NOT FOUND\"\n","            second_last_resistence = \"NOT FOUND\"\n","            last_resistence= lista[0]\n","        elif posicion==1:\n","            second_last_support = \"NOT FOUND\"\n","            last_resistence= \"NOT FOUND\"\n","            second_last_resistence = \"NOT FOUND\"\n","            last_support = lista[0]\n","    elif len(lista) == 2:\n","        if posicion == 0:\n","            last_support = \"NOT FOUND\"\n","            second_last_support = \"NOT FOUND\"\n","            last_resistence= lista[posicion]\n","            second_last_resistence = lista[posicion+1]\n","        elif posicion == 1:\n","            second_last_support = \"NOT FOUND\"\n","            second_last_resistence = \"NOT FOUND\"\n","            last_support = lista[posicion-1]\n","            last_resistence= lista[posicion]\n","        elif posicion == 2:\n","            last_support = \"NOT FOUND\"\n","            second_last_support = \"NOT FOUND\"\n","            last_resistence= \"NOT FOUND\"\n","            second_last_resistence = \"NOT FOUND\"\n","            last_support = lista[posicion-1]\n","            second_last_support = lista[posicion-2]\n","    elif len(lista) == 3:\n","        if posicion == 0:\n","            last_support = \"NOT FOUND\"\n","            second_last_support = \"NOT FOUND\"\n","            last_resistence= lista[posicion]\n","            second_last_resistence = lista[posicion+1]\n","        elif posicion == 1:\n","            second_last_support = \"NOT FOUND\"\n","            last_support = lista[posicion-1]\n","            last_resistence= lista[posicion]\n","            second_last_resistence = lista[posicion+1]\n","        elif posicion == 2:\n","            second_last_resistence = \"NOT FOUND\"\n","            second_last_support = lista[posicion-2]\n","            last_support = lista[posicion-1]\n","            last_resistence= lista[posicion]\n","        elif posicion == 3:\n","            last_resistence= \"NOT FOUND\"\n","            second_last_resistence = \"NOT FOUND\"\n","            last_support = lista[posicion-1]\n","            second_last_support = lista[posicion-2]\n","    elif len(lista) \u003e 3:\n","        if posicion == 0:\n","            last_support= \"NOT FOUND\"\n","            second_last_support = \"NOT FOUND\"\n","            last_resistence= lista[posicion]\n","            second_last_resistence = lista[posicion+1]\n","        elif posicion == len(lista):\n","            last_resistence= \"NOT FOUND\"\n","            second_last_resistence = \"NOT FOUND\"\n","            last_support = lista[posicion-1]\n","            second_last_support = lista[posicion-2]\n","        elif posicion == 1:\n","            second_last_support = \"NOT FOUND\"\n","            last_support = lista[posicion-1]\n","            last_resistence= lista[posicion]\n","            second_last_resistence = lista[posicion+1]\n","        elif posicion == len(lista)-1:\n","            second_last_resistence = \"NOT FOUND\"\n","            last_support = lista[posicion-1]\n","            second_last_support = lista[posicion-2]\n","            last_resistence= lista[posicion]\n","        else:\n","            last_support = lista[posicion-1]\n","            second_last_support = lista[posicion-2]\n","            last_resistence= lista[posicion]\n","            second_last_resistence = lista[posicion+1]\n","    list_of_resistences=[second_last_support, last_support, last_resistence, second_last_resistence]\n","    return(list_of_resistences)\n","\n","def last_support_unification(last_support, \\\n","                             second_last_support_200, last_support_200, \\\n","                             last_resistence, second_last_resistence):\n","    list_of_supports = [second_last_support_200, last_support_200]\n","    list_minimum=[]\n","    if last_support == 'NOT FOUND':\n","        if last_resistence != 'NOT FOUND':\n","            for i in list_of_supports:\n","                if i != 'NOT FOUND':\n","                    if i \u003c int(last_resistence):\n","                        list_minimum.append(i)\n","            if len(list_minimum)\u003e0:\n","                out =  max(list_minimum)\n","            else:\n","                out=-1\n","        else:\n","            for i in list_of_supports:\n","                if i != 'NOT FOUND':\n","                    list_minimum.append(i)\n","            if len(list_minimum)\u003e0:\n","                out =  (max(list_minimum))\n","            else:\n","                out=-2\n","    else:\n","        out = last_support\n","    return out\n","\n","def second_last_support_unification(new_last_support, second_last_support, last_support, \\\n","                             second_last_support_200, last_support_200, \\\n","                             last_resistence, second_last_resistence):\n","    list_of_supports = [second_last_support_200, last_support_200]\n","    list_minimum=[]\n","    if second_last_support == 'NOT FOUND':\n","        if last_resistence != 'NOT FOUND':\n","            for i in list_of_supports:\n","                if i != 'NOT FOUND':\n","                    if i \u003c new_last_support and  i \u003c int(last_resistence):\n","                        list_minimum.append(i)\n","            if len(list_minimum)\u003e0:\n","                out =  max(list_minimum)\n","            else:\n","                out=-1\n","        else:\n","            for i in list_of_supports:\n","                if i != 'NOT FOUND':\n","                     if i \u003c new_last_support:\n","                            list_minimum.append(i)\n","            if len(list_minimum)\u003e0:\n","                out =  (max(list_minimum))\n","            else:\n","                out=-1\n","    else:\n","        out = second_last_support\n","    return out\n","\n","def last_resistence_unification(last_resistence, \\\n","                             second_last_resistence_200, last_resistence_200, \\\n","                             last_support):\n","    list_of_resistences = [second_last_resistence_200, last_resistence_200]\n","    list_minimum=[]\n","    if last_resistence == 'NOT FOUND':\n","        if last_support != 'NOT FOUND':\n","            for i in list_of_resistences:\n","                if i != 'NOT FOUND':\n","                    if i \u003e last_support:\n","                        list_minimum.append(i)\n","            if len(list_minimum)\u003e0:\n","                out =  min(list_minimum)\n","            else:\n","                out=-1\n","        else:\n","            for i in list_of_resistences:\n","                if i != 'NOT FOUND':\n","                    list_minimum.append(i)\n","            if len(list_minimum)\u003e0:\n","                out =  (min(list_minimum))\n","            else:\n","                out=-2\n","    else:\n","        out = last_resistence\n","    return out\n","\n","def second_last_resistence_unification(new_last_resistence, second_last_resistence, last_resistence, \\\n","                             second_last_resistence_200, last_resistence_200, last_support):\n","    list_of_resistences = [second_last_resistence_200, last_resistence_200]\n","    list_minimum=[]\n","    if second_last_resistence == 'NOT FOUND':\n","        if last_support != 'NOT FOUND':\n","            for i in list_of_resistences:\n","                if i != 'NOT FOUND':\n","                    if i \u003e new_last_resistence and  i \u003e last_support:\n","                        list_minimum.append(i)\n","            if len(list_minimum)\u003e0:\n","                out =  min(list_minimum)\n","            else:\n","                out=-1\n","        else:\n","            for i in list_of_resistences:\n","                if i != 'NOT FOUND':\n","                     if i \u003e new_last_resistence:\n","                            list_minimum.append(i)\n","            if len(list_minimum)\u003e0:\n","                out =  (min(list_minimum))\n","            else:\n","                out=-2\n","    else:\n","        out = second_last_resistence\n","    return out\n","\n","def fill_support_not_found_values_in_sr(df, column, measure_column='close', value_to_clean=-1):\n","    df_out=df.copy()\n","    for i in range(len(df_out)):\n","        if df_out.iloc[i][column] == value_to_clean and df_out.iloc[i][measure_column] != value_to_clean:\n","            for var in range(i+1,len(df_out)):\n","                if (df_out.iloc[var][column] \u003e -1) and (df_out.iloc[var][column] \u003c df_out.iloc[i][measure_column]):\n","                    df_out.at[i, column] = df_out.iloc[var][column]\n","                    break\n","    return df_out\n","\n","def fill_resistence_not_found_values_in_sr(df, column, measure_column='close', value_to_clean=-1):\n","    df_out=df.copy()\n","    for i in range(len(df_out)):\n","        if df_out.iloc[i][column] == value_to_clean and df_out.iloc[i][measure_column] != value_to_clean:\n","            for var in range(i+1,len(df_out)):\n","                if (df_out.iloc[var][column] \u003e -1) and (df_out.iloc[var][column] \u003e df_out.iloc[i][measure_column]):\n","                    df_out.at[i, column] = df_out.iloc[var][column]\n","                    break\n","    return df_out\n","\n","def get_supports_and_resistences(df, close_column='close', date_column='date'):\n","    df_sr  =supports_and_resistences(df, look_back=100)\n","    df_sr_200 = supports_and_resistences_more_days(df, 200, window_=10)\n","    df_close = df[['date', 'close']].copy()\n","    df_sr_total = pd.merge(df_sr, df_sr_200, on='date', how='left')\n","    df_sr_total.fillna('NOT FOUND', inplace=True)\n","    df_sr_total = pd.merge(df_sr_total,df_close,on='date',how='inner')\n","    df_sr_total['new_last_support'] = df_sr_total[['last_support', 'second_last_support_200', 'last_support_200',\\\n","                                          'last_resistence','second_last_resistence']].apply(lambda x : last_support_unification(*x), axis=1)\n","    df_sr_total['new_second_last_support'] = df_sr_total[['new_last_support', 'second_last_support','last_support', 'second_last_support_200', 'last_support_200',\\\n","                                          'last_resistence','second_last_resistence']].apply(lambda x : second_last_support_unification(*x), axis=1)\n","    df_sr_total['new_last_resistence'] = df_sr_total[['last_resistence', 'second_last_resistence_200', 'last_resistence_200',\\\n","                                          'last_support']].apply(lambda x : last_resistence_unification(*x), axis=1)\n","    df_sr_total['new_second_last_resistence'] = df_sr_total[['new_last_resistence', 'second_last_resistence', 'last_resistence', 'second_last_resistence_200', 'last_resistence_200',\\\n","                                          'last_support']].apply(lambda x : second_last_resistence_unification(*x), axis=1)\n","    df_sr_total = df_sr_total[['date', 'close', 'new_second_last_resistence', 'new_last_resistence', 'new_second_last_support', 'new_last_support']]\n","    df_filled = df_sr_total.copy()\n","    df_filled = fill_support_not_found_values_in_sr(df_filled, 'new_last_support')\n","    df_filled = fill_support_not_found_values_in_sr(df_filled, 'new_second_last_support', measure_column='new_last_support')\n","    df_filled = fill_resistence_not_found_values_in_sr(df_filled, 'new_last_resistence')\n","    df_filled = fill_resistence_not_found_values_in_sr(df_filled, 'new_second_last_resistence', measure_column='new_last_resistence')\n","    df_filled = df_filled.rename(columns={'new_second_last_resistence':'second_last_resistence',\\\n","                                         'new_last_resistence':'last_resistence',\\\n","                                         'new_last_support':'last_support',\\\n","                                         'new_second_last_support':'second_last_support'})\n","    df_out = df_filled\n","\n","    for i in range(len(df_out)):\n","        df_it_out = df_out[i:len(df_out)]\n","        maximum_value = df_it_out.close.max()\n","        minimum_value = df_it_out.close.min()\n","        if df_out[['last_support']].iloc[i][0]==-1:\n","            df_out.at[i, 'last_support'] = minimum_value\n","        if df_out[['second_last_support']].iloc[i][0]==-1:\n","            df_out.at[i, 'second_last_support'] = minimum_value\n","        if df_out[['last_resistence']].iloc[i][0]==-1:\n","            df_out.at[i, 'last_resistence'] = maximum_value\n","        if df_out[['second_last_resistence']].iloc[i][0]==-1:\n","            df_out.at[i, 'second_last_resistence'] = maximum_value\n","    df_out =df_out.drop(columns=close_column, axis=1)\n","\n","    return df_out\n","\n","def getHigherLows(data: np.array, order=5, K=2):\n","    '''\n","    Finds consecutive higher lows in price pattern.\n","    Must not be exceeded within the number of periods indicated by the width\n","    parameter for the value to be confirmed.\n","    K determines how many consecutive lows need to be higher.\n","    '''\n","    # Get lows\n","    low_idx = argrelextrema(data, np.less, order=order)[0]\n","    lows = data[low_idx]\n","    # Ensure consecutive lows are higher than previous lows\n","    extrema = []\n","    ex_deque = deque(maxlen=K)\n","    for i, idx in enumerate(low_idx):\n","        if i == 0:\n","            ex_deque.append(idx)\n","            continue\n","        if lows[i] \u003c lows[i-1]:\n","            ex_deque.clear()\n","\n","        ex_deque.append(idx)\n","        if len(ex_deque) == K:\n","            extrema.append(ex_deque.copy())\n","\n","    return extrema\n","\n","def getLowerHighs(data: np.array, order=5, K=2):\n","    '''\n","    Finds consecutive lower highs in price pattern.\n","    Must not be exceeded within the number of periods indicated by the width\n","    parameter for the value to be confirmed.\n","    K determines how many consecutive highs need to be lower.\n","    '''\n","    # Get highs\n","    high_idx = argrelextrema(data, np.greater, order=order)[0]\n","    highs = data[high_idx]\n","    # Ensure consecutive highs are lower than previous highs\n","    extrema = []\n","    ex_deque = deque(maxlen=K)\n","    for i, idx in enumerate(high_idx):\n","        if i == 0:\n","            ex_deque.append(idx)\n","            continue\n","        if highs[i] \u003e highs[i-1]:\n","            ex_deque.clear()\n","\n","        ex_deque.append(idx)\n","        if len(ex_deque) == K:\n","            extrema.append(ex_deque.copy())\n","\n","    return extrema\n","\n","def getHigherHighs(data: np.array, order=5, K=2):\n","    '''\n","    Finds consecutive higher highs in price pattern.\n","    Must not be exceeded within the number of periods indicated by the width\n","    parameter for the value to be confirmed.\n","    K determines how many consecutive highs need to be higher.\n","    '''\n","    # Get highs\n","    high_idx = argrelextrema(data, np.greater, order=5)[0]\n","    highs = data[high_idx]\n","    # Ensure consecutive highs are higher than previous highs\n","    extrema = []\n","    ex_deque = deque(maxlen=K)\n","    for i, idx in enumerate(high_idx):\n","        if i == 0:\n","            ex_deque.append(idx)\n","            continue\n","        if highs[i] \u003c highs[i-1]:\n","            ex_deque.clear()\n","\n","        ex_deque.append(idx)\n","        if len(ex_deque) == K:\n","            extrema.append(ex_deque.copy())\n","\n","    return extrema\n","\n","def getLowerLows(data: np.array, order=5, K=2):\n","    '''\n","    Finds consecutive lower lows in price pattern.\n","    Must not be exceeded within the number of periods indicated by the width\n","    parameter for the value to be confirmed.\n","    K determines how many consecutive lows need to be lower.\n","    '''\n","    # Get lows\n","    low_idx = argrelextrema(data, np.less, order=order)[0]\n","    lows = data[low_idx]\n","    # Ensure consecutive lows are lower than previous lows\n","    extrema = []\n","    ex_deque = deque(maxlen=K)\n","    for i, idx in enumerate(low_idx):\n","        if i == 0:\n","            ex_deque.append(idx)\n","            continue\n","        if lows[i] \u003e lows[i-1]:\n","            ex_deque.clear()\n","\n","        ex_deque.append(idx)\n","        if len(ex_deque) == K:\n","            extrema.append(ex_deque.copy())\n","\n","        return extrema\n","\n","# no se usan\n","def getHHIndex(data: np.array, order=150, K=2):\n","    extrema = getHigherHighs(data, order, K)\n","    print(extrema)\n","    idx = np.array([i[-1] + order for i in extrema])\n","    print(idx)\n","    return idx[np.where(idx\u003clen(data))]\n","def getLHIndex(data: np.array, order=5, K=2):\n","    extrema = getLowerHighs(data, order, K)\n","    idx = np.array([i[-1] + order for i in extrema])\n","    return idx[np.where(idx\u003clen(data))]\n","def getLLIndex(data: np.array, order=5, K=2):\n","    extrema = getLowerLows(data, order, K)\n","    idx = np.array([i[-1] + order for i in extrema])\n","    return idx[np.where(idx\u003clen(data))]\n","def getHLIndex(data: np.array, order=5, K=2):\n","    extrema = getHigherLows(data, order, K)\n","    idx = np.array([i[-1] + order for i in extrema])\n","    return idx[np.where(idx\u003clen(data))]\n","\n","\n","def get_hh_divergences_paterns(close_t, function, a, days=5):\n","    if function == getHigherHighs:\n","        inc=0\n","    elif function == getLowerLows:\n","        inc=10000000000000000000000000000000000000\n","    elif function == getHigherLows:\n","        inc=10000000000000000000000000000000000000\n","    elif function == getLowerHighs:\n","        inc=0\n","    #treatement\n","    close_t = list(close_t)\n","    close_t.append(inc)\n","    close_t = np.asarray(close_t)\n","    hhp = function(close_t)\n","    hhp_temp=0\n","\n","    try:\n","        hhp_temp = hhp[-1][1]\n","    except:\n","        pass\n","    if a-hhp_temp \u003c days:\n","        hh_value=1\n","    else:\n","        hh_value=0\n","    # output hh_value\n","    return(hh_value)\n","\n","def get_divergences_points(df, indicator='close', date_column='date', days_init=5):\n","    df = df.sort_values([date_column], ascending=[False]).reset_index(drop=True).sort_values([date_column], ascending=[True]).copy()\n","    df = df[[date_column, indicator]]\n","    df = df.dropna()\n","    close = df[indicator].values\n","    dates = df[date_column].values\n","    hh = []\n","    hl = []\n","    ll = []\n","    lh = []\n","    for i, k in enumerate(dates):\n","        # input\n","        close_temp=close[:i+1]\n","        # 1.1 higher highs\n","        hh.append(get_hh_divergences_paterns(close_temp, getHigherHighs, i))\n","        #1.2 lower lows\n","        ll.append(get_hh_divergences_paterns(close_temp, getLowerLows, i))\n","        #1.3 Higher lows\n","        hl.append(get_hh_divergences_paterns(close_temp, getHigherLows, i))\n","        #1.4 Lower Highs\n","        lh.append(get_hh_divergences_paterns(close_temp, getLowerHighs, i))\n","\n","    for i in range(days_init):\n","        hh[i]=0\n","        ll[i]=0\n","        hl[i]=0\n","        lh[i]=0\n","    data = pd.DataFrame(columns = [date_column, 'hh_'+indicator, 'll_'+indicator, 'hl_'+indicator, 'lh_'+indicator])\n","    df_final = data.append(pd.DataFrame({date_column : dates, 'hh_'+indicator:hh, 'll_'+indicator:ll, 'hl_'+indicator:hl, 'lh_'+indicator:lh}))\n","\n","    return(df_final)\n","\n","\n","\n","def add_derivates(df, feature, periods=1):\n","    df_new = df.copy()\n","    diff_1 = df[[feature]].diff(periods=periods).rename(columns={feature: feature +\"_diff1\"})\n","    df_new = pd.concat([df_new, diff_1], axis=1).dropna()\n","    del df\n","    return df_new\n","\n","def get_ribbons(df, sma_low, sma_high, list_of_features, date_column='date'):\n","    df_out=df.copy()\n","    list_of_columns = [date_column] + list_of_features\n","    df_out=df_out[list_of_columns]\n","    list_of_sma = []\n","    list_of_ribbon = []\n","    for i in list_of_features:\n","        sma_low_ = i + '_sma_' + str(sma_low)\n","        list_of_sma.append(sma_low_)\n","        df_out[sma_low_] = pd.Series.to_frame(ta.sma(df_out[i], length=sma_low))\n","        sma_high_ = i + '_sma_' + str(sma_high)\n","        list_of_sma.append(sma_high_)\n","        df_out[sma_high_] = pd.Series.to_frame(ta.sma(df_out[i], length=sma_high))\n","        ribbon_ = 'ribbon_' + i + '_' +  str(sma_low) + '_' + str(sma_high)\n","        list_of_ribbon.append(ribbon_)\n","        df_out[ribbon_] = (df_out[sma_low_] - df_out[sma_high_])\n","    df_out = df_out.dropna()\n","    return df_out\n","\n","def get_ribbons_30_60(df, list_of_names_df):\n","    df_final=df.copy()\n","    list_of_sma = []\n","    list_of_ribbon = []\n","    for i in list_of_names_df:\n","        sma_30 = i + '_sma_30'\n","        list_of_sma.append(sma_30)\n","        df_final[sma_30] = pd.Series.to_frame(ta.sma(df_final[i], length=30))\n","        sma_60 = i + '_sma_60'\n","        list_of_sma.append(sma_60)\n","        df_final[sma_60] = pd.Series.to_frame(ta.sma(df_final[i], length=60))\n","        ribbon_30_60 = 'ribbon_' + i + '_30_60'\n","        list_of_ribbon.append(ribbon_30_60)\n","        df_final[ribbon_30_60] = (df_final[sma_30] - df_final[sma_60])\n","    return df_final\n","\n","def create_linear_regression_of_maximums(df_in, feature_column, date_column='date'):\n","\n","    df_out = df_in[[date_column, feature_column]]\n","    if date_column != 'date':\n","        df_out = df_out.rename(columns={date_column:'date'})\n","    date_column='date'\n","    first_date_str = df_out.iloc[0].date\n","    first_date = first_date_str.to_pydatetime().date()\n","    df_out[df_out[date_column]==first_date_str].iloc[0][1]#.CPTRA_sma_30\n","    maximum_date_CPTRA = {}\n","    today = date.today()\n","    number_of_years = today.year - first_date.year\n","    low_date = first_date\n","    maximum_value=0\n","    list_of_dates=[]\n","    list_of_values=[]\n","\n","    for i in range(number_of_years+1):\n","        low_date =  first_date_str + dt.timedelta(days = 365*i)\n","        high_date =  first_date_str + dt.timedelta(days = 365*(i+1))\n","        df_filtered = df_out[(df_out[date_column] \u003e low_date) \u0026  (df_out[date_column] \u003c= high_date)]\n","        maximum = df_filtered[[feature_column]].max()[0]#.CPTRA_sma_30\n","        if maximum \u003e maximum_value:\n","            maximum_value = maximum\n","            list_of_dates.append(df_filtered[df_filtered[feature_column]==df_filtered[feature_column].max()].reset_index().iloc[0].date)\n","            list_of_values.append(maximum)\n","\n","    dataframe_dates_maximum = pd.DataFrame(\n","        {date_column: list_of_dates,\n","        feature_column: list_of_values\n","        })\n","\n","    total_days = (today - first_date).days\n","    # https://laid-back-scientist.com/en/fill-datetime\n","    df_ = dataframe_dates_maximum.set_index(date_column)\n","    df_ = df_.asfreq(freq='1440min')\n","    df_fill = df_.reset_index()\n","\n","    # https://mohammadimranhasan.com/linear-regression-of-time-series-data-with-pandas-library-in-python/\n","    df_dates = df_out[[date_column]].copy()\n","    df_new = pd.merge(df_dates, dataframe_dates_maximum, how='left', on=date_column)\n","    df_fill = df_new.copy()\n","    df_fill =df_fill.set_index(date_column)\n","    y=np.array(df_fill[feature_column].dropna().values, dtype=float)\n","    x=np.array(pd.to_datetime(df_fill[feature_column].dropna()).index.values, dtype=float)\n","    x_tot = np.array(pd.to_datetime(df_fill[feature_column]).index.values, dtype=float)\n","    slope, intercept, r_value, p_value, std_err =sp.linregress(x,y)\n","    xf = np.linspace(min(x_tot),max(x_tot),len(df_fill))\n","    xf1 = xf.copy()\n","    xf1 = pd.to_datetime(xf1)\n","    yf = (slope*xf)+intercept\n","    x_for_df = xf1.to_list()\n","    y_for_df = np.array(yf).tolist()\n","    name_of_feature = 'linear_regression_max_' + feature_column\n","    df_linear_regression_max = pd.DataFrame(\n","        {date_column: x_for_df,\n","        name_of_feature: y_for_df\n","        })\n","    df_linear_regression_max = df_linear_regression_max[df_linear_regression_max['linear_regression_max_' + feature_column]\u003e0]\n","    df_linear_regression_max[date_column] = pd.to_datetime(df_linear_regression_max[date_column]).dt.date\n","    df_linear_regression_max[date_column] = pd.to_datetime(df_linear_regression_max[date_column])\n","    #df_linear_regression_max[name_of_feature] = df_linear_regression_max[name_of_feature].clip(lower=0)\n","\n","    return df_linear_regression_max\n","\n","def create_linear_regression_of_minimums(df_in, feature_column, date_column='date'):\n","\n","    df_out = df_in[[date_column,feature_column]].copy()\n","\n","    first_date_str = df_out.iloc[0].date\n","    first_date = first_date_str.to_pydatetime().date()\n","    df_out[df_out[date_column]==first_date_str].iloc[0].CPTRA_sma_30\n","    maximum_date_CPTRA = {}\n","    today = date.today()\n","    number_of_years = today.year - first_date.year\n","    low_date = first_date\n","    maximum_value=0\n","    list_of_dates=[]\n","    list_of_values=[]\n","\n","    for i in range(number_of_years+1):\n","        low_date =  first_date_str + dt.timedelta(days = 365*i)\n","        high_date =  first_date_str + dt.timedelta(days = 365*(i+1))\n","        df_filtered = df_out[(df_out[date_column] \u003e low_date) \u0026  (df_out[date_column] \u003c= high_date)]\n","        minimum = df_filtered[[feature_column]].min().CPTRA_sma_30\n","        try:\n","            list_of_dates.append(df_filtered[df_filtered[feature_column]==df_filtered[feature_column].min()].reset_index().iloc[0].date)\n","            list_of_values.append(minimum)\n","        except:\n","            pass\n","\n","    # Now get minimum absolutes\n","\n","    list_of_absolutes_dates=[]\n","    list_of_absolutes_values=[]\n","    count=0\n","    length_data = len(list_of_values)\n","    maximum_of_minimum = list_of_values[length_data-1]\n","    list_of_absolutes_values.append(list_of_values[length_data-1])\n","    list_of_absolutes_dates.append(list_of_dates[length_data-1])\n","    for i in reversed(list_of_values):\n","        count+=1\n","        if i \u003c maximum_of_minimum:\n","            maximum_of_minimum = i\n","            list_of_absolutes_values.append(i)\n","            list_of_absolutes_dates.append(list_of_dates[length_data-count])\n","\n","    dataframe_dates_minimum = pd.DataFrame(\n","        {date_column: list_of_absolutes_dates,\n","        feature_column: list_of_absolutes_values\n","        })\n","    dataframe_dates_minimum = dataframe_dates_minimum[:-1] # delete last data\n","    total_days = (today - first_date).days\n","    df_ = dataframe_dates_minimum.set_index(date_column)\n","    df_ = df_.asfreq(freq='1440min')\n","    df_fill = df_.reset_index()\n","    df_dates = df_out[[date_column]]\n","    df_new = pd.merge(df_dates, dataframe_dates_minimum, how='left', on=date_column)\n","    df_fill = df_new.copy()\n","    df_fill =df_fill.set_index(date_column)\n","\n","    # https://mohammadimranhasan.com/linear-regression-of-time-series-data-with-pandas-library-in-python/\n","    df_dates = df_out[[date_column]].copy()\n","    df_new = pd.merge(df_dates, dataframe_dates_minimum, how='left', on=date_column)\n","    df_fill = df_new.copy()\n","    df_fill =df_fill.set_index(date_column)\n","    y=np.array(df_fill[feature_column].dropna().values, dtype=float)\n","    x=np.array(pd.to_datetime(df_fill[feature_column].dropna()).index.values, dtype=float)\n","    x_tot = np.array(pd.to_datetime(df_fill[feature_column]).index.values, dtype=float)\n","    slope, intercept, r_value, p_value, std_err =sp.linregress(x,y)\n","    xf = np.linspace(min(x_tot),max(x_tot),len(df_fill))\n","    xf1 = xf.copy()\n","    xf1 = pd.to_datetime(xf1)\n","    yf = (slope*xf)+intercept\n","    x_for_df = xf1.to_list()\n","    y_for_df = np.array(yf).tolist()\n","    name_of_feature = 'linear_regression_min_' + feature_column\n","    df_linear_regression_min = pd.DataFrame(\n","        {date_column: x_for_df,\n","        name_of_feature : y_for_df\n","        })\n","    df_linear_regression_min = df_linear_regression_min[df_linear_regression_min['linear_regression_min_' + feature_column]\u003e0]\n","    df_linear_regression_min[date_column] = pd.to_datetime(df_linear_regression_min[date_column]).dt.date\n","    df_linear_regression_min[date_column] = pd.to_datetime(df_linear_regression_min[date_column])\n","    df_linear_regression_min[name_of_feature] = df_linear_regression_min[name_of_feature].clip(lower=0)\n","    return df_linear_regression_min\n","\n","def create_list_of_columns(df, datecolumn='date'):\n","    list_out = []\n","    for i in df.columns:\n","        if i != datecolumn:\n","            list_out.append(i)\n","    return list_out\n","\n","def obtener_combinaciones(lista):\n","    resultados = []\n","    for i in range(1, len(lista) + 1):\n","        combos = combinations(lista, i)\n","        resultados.extend(list(combos))\n","\n","    # Convertir cada tupla en una lista\n","    resultados = [list(combo) for combo in resultados]\n","    return resultados\n","\n","def create_model(X_train, input_layers = 4, units=200, drop=0.1):\n","    model = Sequential()\n","    for i in range(input_layers-1):\n","        model.add(LSTM(units = units, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n","        model.add(Dropout(drop))\n","    model.add(Dropout(drop))\n","    model.add(LSTM(units = units))\n","    #model.add(Dense(1)) #y_train.shape[-1])\n","    model.add(Dense(1, activation='sigmoid'))\n","    return(model)\n","\n","def create_dataset(dataset, dataset_y, look_back, look_forward):\n","    dataX, dataY = [], []\n","    for i in range(len(dataset)-look_back-look_forward):\n","        a = dataset[i:(i+look_back)]\n","        dataX.append(a)\n","        dataY.append(dataset_y[(i + look_back+ look_forward-1):(i + look_back + look_forward), 0]) #ojo look_forward primero\n","    return np.array(dataX), np.array(dataY)\n","\n","def create_dataset_official(dataset, dataset_y, look_back):\n","    dataX, dataY = [], []\n","    for i in range(look_back,len(dataset)):\n","        a = dataset[i-look_back:i]\n","        dataX.append(a)\n","        dataY.append(dataset_y[i])\n","    return np.array(dataX), np.array(dataY)\n","\n","def create_dataset_X_y(dataset_X, dataset_y, look_back, test_size):\n","    pos = int(round(len(dataset_X) * (1-test_size)))\n","    data_train_X, data_train_y, data_test_X, data_test_y = [], [], [], []\n","    for i in range(look_back,pos):\n","        a = dataset_X[i-look_back:i]\n","        data_train_X.append(a)\n","        data_train_y.append(dataset_y[i])\n","    for i in range(pos,len(dataset_X)):\n","        a = dataset_X[i-look_back:i]\n","        data_test_X.append(a)\n","        data_test_y.append(dataset_y[i])\n","    return np.array(data_train_X), np.array(data_train_y), np.array(data_test_X), np.array(data_test_y)\n","\n","def split_data_dates(dates_, test_size):\n","        pos = int(round(len(dates_) * (1-test_size)))\n","        dates_train = dates_[:pos]\n","        dates_test = dates_[pos:]\n","        return dates_train, dates_test\n","\n","def split_data(X, y, test_size):\n","        pos = int(round(len(X) * (1-test_size)))\n","        X_train, y_train = X[:pos], y[:pos]\n","        X_test, y_test = X[pos:], y[pos:]\n","        return X_train, y_train, X_test, y_test\n","\n","def test_lstm(df_in, capas=4, epochs=20, look_back=10, look_forward=0, batch_size=20, split=0.2,\n","              drop=0.1, optimizador='sgd', initial_days=1, days_target=10, column_target = 'target', units=50, column_date='date'):\n","    #df_lstm = df.sort_index(ascending=False)\n","    dates = df_in[column_date]\n","    dates_train, dates_test = split_data_dates(dates, split)\n","\n","    X=df_in.drop(columns=['target',column_date], axis=1)\n","    #Y será la clase a predecir\n","\n","    y=df_in['target']\n","\n","    #dataset= X.values\n","    dataset_y = y.values\n","    df_for_scaled = pd.concat([X,y], axis=1)\n","    dataset = df_for_scaled.values\n","    dataset = dataset.reshape(-1,dataset[0].size)\n","    dataset = dataset.astype(\"float32\")\n","\n","    #dataset_y = dataset_y.reshape(-1,1)\n","    dataset_y = dataset_y.astype(\"float32\")\n","\n","    # scaling datase\n","    scaler = MinMaxScaler(feature_range=(0,1))\n","    dataset = scaler.fit_transform(dataset)\n","    #dataset_y = scaler.fit_transform(dataset_y)\n","    #X, y = create_dataset(dataset, dataset_y, look_back,look_forward)\n","    X, y = create_dataset_official(dataset, dataset_y, look_back)\n","    print(\"X:{},y:{}\".format(X.shape, y.shape))\n","    X_train, y_train, X_test, y_test = split_data(X, y, split)\n","    print(\"X_train:{},y_train:{},X_test:{},y_test{}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n","    # Initialising the LSTM\n","    model = create_model(X_train, input_layers=capas)\n","    model.compile(optimizer = optimizador, loss = 'mean_squared_error')\n","    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n","\n","    #### EXECUTING THE MODEL ####\n","    # predictions\n","    trainPredict = model.predict(X_train)\n","    testPredict = model.predict(X_test)\n","\n","    # reshape\n","    #trainPredict = scaler.inverse_transform(trainPredict)\n","    #trainY = scaler.inverse_transform(y_train)\n","    #testPredict = scaler.inverse_transform(testPredict)\n","    #testY = scaler.inverse_transform(y_test)\n","    testY = y_test\n","    trainY = y_train\n","    # metrics\n","    #trainY=trainY.reshape(1,-1)\n","    #testY = testY.reshape(1,-1)\n","    # Calcular AUC\n","    auc_test = roc_auc_score(testY, testPredict)\n","    auc_train = roc_auc_score(trainY, trainPredict)\n","    acc_test = accuracy_score(testY, testPredict.round())\n","    acc_train = accuracy_score(trainY, trainPredict.round())\n","    return(auc_test, auc_train, acc_test, acc_train, len(X_train), len(X_test), testPredict, dates_test)\n","\n","\n","def lstm(df_in, ticker, capas=3, epochs=60, look_back=25, batch_size=20, split=0.2,\n","              drop=0.1, optimizador='sgd', initial_days=1, days_target=10, column_target = 'target', units=50, column_date='date'):\n","    #df_lstm = df.sort_index(ascending=False)\n","    dates = df_in['date']\n","    dates_train, dates_test = split_data_dates(dates, split)\n","    X=df_in.drop(columns=['target',column_date], axis=1)\n","    #Y será la clase a predecir\n","    y=df_in[column_target]\n","    #dataset= X.values\n","    dataset_y = y.values\n","    df_for_scaled = pd.concat([X,y], axis=1)\n","    dataset = df_for_scaled.values\n","    dataset = dataset.reshape(-1,dataset[0].size)\n","    dataset = dataset.astype(\"float32\")\n","    #dataset_y = dataset_y.reshape(-1,1)\n","    dataset_y = dataset_y.astype(\"float32\")\n","\n","    # scaling datase\n","    scaler = MinMaxScaler(feature_range=(0,1))\n","    dataset = scaler.fit_transform(dataset)\n","    #dataset_y = scaler.fit_transform(dataset_y)\n","    #X, y = create_dataset(dataset, dataset_y, look_back,look_forward)\n","    X_train, y_train, X_test, y_test = create_dataset_X_y(X, dataset_y, look_back, split)\n","    print(\"X_train:{},y_train:{},X_test:{},y_test{}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n","    # Initialising the LSTM\n","    model = create_model(X_train, input_layers=capas)\n","    model.compile(optimizer = optimizador, loss = 'mean_squared_error')\n","    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n","    #model.save('model_'+ ticker + '.h5')\n","    #### EXECUTING THE MODEL ####\n","    # predictions\n","    trainPredict = model.predict(X_train)\n","    testPredict = model.predict(X_test)\n","    # reshape\n","    #trainPredict = scaler.inverse_transform(trainPredict)\n","    #trainY = scaler.inverse_transform(y_train)\n","    #testPredict = scaler.inverse_transform(testPredict)\n","    #testY = scaler.inverse_transform(y_test)\n","    testY = y_test\n","    trainY = y_train\n","    # metrics\n","    #trainY=trainY.reshape(1,-1)\n","    #testY = testY.reshape(1,-1)\n","    # Calcular AUC\n","    auc_test = roc_auc_score(testY, testPredict)\n","    auc_train = roc_auc_score(trainY, trainPredict)\n","    acc_test = accuracy_score(testY, testPredict.round())\n","    acc_train = accuracy_score(trainY, trainPredict.round())\n","    testPredict_flat = [val[0] for val in testPredict]\n","    # Crear un DataFrame combinando 'testPredict' y 'dates_test'\n","    data = {'dates_test': dates_test, 'testPredict': testPredict_flat, 'testY':testY}\n","    df_out = pd.DataFrame(data)\n","    return(auc_test, auc_train, acc_test, acc_train, df_out)\n","\n","columns_to_get_percentage = ['SMA_20',  'EMA_20_OHLC4', 'SMA_55', 'EMA_55_OHLC4', 'EMA_200_OHLC4', 'ISA_9',\n","       'ISB_26', 'ITS_9', 'IKS_26', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0',\n","       'SQZ_20_2.0_20_1.5', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9']\n","\n","def create_indicators_percentages(df_in, columns_to_get_percentaje, close_column='close'):\n","    df_out = df_in.copy()\n","    for i in columns_to_get_percentaje:\n","        df_out[i] = 100*((df_out[close_column] - df_out[i])/df_out[close_column])\n","    return(df_out)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"voViWStcsnHv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","1094/1094 [==============================] - 22s 10ms/step - loss: 0.2488\n","Epoch 2/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2481\n","Epoch 3/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2477\n","Epoch 4/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2474\n","Epoch 5/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2473\n","Epoch 6/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2472\n","Epoch 7/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2470\n","Epoch 8/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2471\n","Epoch 9/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2467\n","Epoch 10/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2469\n","Epoch 11/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2467\n","Epoch 12/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2467\n","Epoch 13/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2465\n","Epoch 14/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2467\n","Epoch 15/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2466\n","Epoch 16/100\n","1094/1094 [==============================] - 7s 6ms/step - loss: 0.2465\n","Epoch 17/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2465\n","Epoch 18/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2463\n","Epoch 19/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2464\n","Epoch 20/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2463\n","Epoch 21/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2464\n","Epoch 22/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2464\n","Epoch 23/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2462\n","Epoch 24/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2463\n","Epoch 25/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2461\n","Epoch 26/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2462\n","Epoch 27/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2462\n","Epoch 28/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2463\n","Epoch 29/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2461\n","Epoch 30/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2461\n","Epoch 31/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2462\n","Epoch 32/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2461\n","Epoch 33/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2460\n","Epoch 34/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2458\n","Epoch 35/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2459\n","Epoch 36/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2460\n","Epoch 37/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2459\n","Epoch 38/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2459\n","Epoch 39/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2460\n","Epoch 40/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2460\n","Epoch 41/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2457\n","Epoch 42/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2458\n","Epoch 43/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2457\n","Epoch 44/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2459\n","Epoch 45/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2457\n","Epoch 46/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2456\n","Epoch 47/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2456\n","Epoch 48/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2456\n","Epoch 49/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2455\n","Epoch 50/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2456\n","Epoch 51/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2456\n","Epoch 52/100\n","1094/1094 [==============================] - 7s 6ms/step - loss: 0.2457\n","Epoch 53/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2456\n","Epoch 54/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2456\n","Epoch 55/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2455\n","Epoch 56/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2457\n","Epoch 57/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2454\n","Epoch 58/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2455\n","Epoch 59/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2457\n","Epoch 60/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2457\n","Epoch 61/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2454\n","Epoch 62/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2453\n","Epoch 63/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2454\n","Epoch 64/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2455\n","Epoch 65/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2454\n","Epoch 66/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2451\n","Epoch 67/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2450\n","Epoch 68/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2453\n","Epoch 69/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2451\n","Epoch 70/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2453\n","Epoch 71/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2452\n","Epoch 72/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2452\n","Epoch 73/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2452\n","Epoch 74/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2452\n","Epoch 75/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2452\n","Epoch 76/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2450\n","Epoch 77/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2448\n","Epoch 78/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2451\n","Epoch 79/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2448\n","Epoch 80/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2448\n","Epoch 81/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2448\n","Epoch 82/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2449\n","Epoch 83/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2447\n","Epoch 84/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2445\n","Epoch 85/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2449\n","Epoch 86/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2447\n","Epoch 87/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2446\n","Epoch 88/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2446\n","Epoch 89/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2444\n","Epoch 90/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2442\n","Epoch 91/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2444\n","Epoch 92/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2448\n","Epoch 93/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2443\n","Epoch 94/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2443\n","Epoch 95/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2443\n","Epoch 96/100\n","1094/1094 [==============================] - 6s 6ms/step - loss: 0.2442\n","Epoch 97/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2440\n","Epoch 98/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2437\n","Epoch 99/100\n","1094/1094 [==============================] - 6s 5ms/step - loss: 0.2439\n","Epoch 100/100\n","1094/1094 [==============================] - 5s 5ms/step - loss: 0.2437\n","684/684 [==============================] - 3s 3ms/step\n","174/174 [==============================] - 0s 2ms/step\n","  epochs layers days_back  auc_test  auc_train  acc_test  acc_train\n","0    100      1        30  0.492447   0.588832  0.499098   0.569319\n","Epoch 1/100\n","1094/1094 [==============================] - 19s 13ms/step - loss: 0.2483\n","Epoch 2/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2482\n","Epoch 3/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2482\n","Epoch 4/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2481\n","Epoch 5/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2480\n","Epoch 6/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2478\n","Epoch 7/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2478\n","Epoch 8/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2477\n","Epoch 9/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2477\n","Epoch 10/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2476\n","Epoch 11/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2475\n","Epoch 12/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2476\n","Epoch 13/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2475\n","Epoch 14/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2475\n","Epoch 15/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2474\n","Epoch 16/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2473\n","Epoch 17/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2473\n","Epoch 18/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2472\n","Epoch 19/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2473\n","Epoch 20/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2472\n","Epoch 21/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2472\n","Epoch 22/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2473\n","Epoch 23/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2471\n","Epoch 24/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2471\n","Epoch 25/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2470\n","Epoch 26/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2470\n","Epoch 27/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2470\n","Epoch 28/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2470\n","Epoch 29/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2470\n","Epoch 30/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2470\n","Epoch 31/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2469\n","Epoch 32/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2468\n","Epoch 33/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2469\n","Epoch 34/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2468\n","Epoch 35/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2468\n","Epoch 36/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2467\n","Epoch 37/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2469\n","Epoch 38/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2466\n","Epoch 39/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2466\n","Epoch 40/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2467\n","Epoch 41/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2467\n","Epoch 42/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2466\n","Epoch 43/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2466\n","Epoch 44/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2467\n","Epoch 45/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2465\n","Epoch 46/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2466\n","Epoch 47/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2465\n","Epoch 48/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2464\n","Epoch 49/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2465\n","Epoch 50/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2464\n","Epoch 51/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2463\n","Epoch 52/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2464\n","Epoch 53/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2463\n","Epoch 54/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2463\n","Epoch 55/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2461\n","Epoch 56/100\n","1094/1094 [==============================] - 15s 13ms/step - loss: 0.2462\n","Epoch 57/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2462\n","Epoch 58/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2462\n","Epoch 59/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2461\n","Epoch 60/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2460\n","Epoch 61/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2461\n","Epoch 62/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2460\n","Epoch 63/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2459\n","Epoch 64/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2461\n","Epoch 65/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2459\n","Epoch 66/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2459\n","Epoch 67/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2459\n","Epoch 68/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2459\n","Epoch 69/100\n","1094/1094 [==============================] - 14s 13ms/step - loss: 0.2458\n","Epoch 70/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2457\n","Epoch 71/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2456\n","Epoch 72/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2457\n","Epoch 73/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2457\n","Epoch 74/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2456\n","Epoch 75/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2458\n","Epoch 76/100\n","1094/1094 [==============================] - 14s 12ms/step - loss: 0.2456\n","Epoch 77/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2457\n","Epoch 78/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2454\n","Epoch 79/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2455\n","Epoch 80/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2455\n","Epoch 81/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2455\n","Epoch 82/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2455\n","Epoch 83/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2452\n","Epoch 84/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2454\n","Epoch 85/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2454\n","Epoch 86/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2454\n","Epoch 87/100\n","1094/1094 [==============================] - 13s 12ms/step - loss: 0.2454\n","Epoch 88/100\n","  59/1094 [\u003e.............................] - ETA: 11s - loss: 0.2470"]}],"source":["\n","test_size=0.2\n","ticker='SAP'\n","capas=4\n","epochs=1\n","look_back=30\n","batch_size=20\n","split=0.2\n","drop=0.1\n","optimizador='sgd'\n","initial_days=1\n","days_target=10\n","column_target = 'target'\n","units=50\n","column_date='date'\n","\n","columnas = ['epochs', 'layers', 'days_back', 'auc_test', 'auc_train', 'acc_test', 'acc_train']\n","# Crear un DataFrame vacío con las columnas\n","df_final = pd.DataFrame(columns=columnas)\n","\n","list_of_companies = ['SAP', 'RACE', 'DTE', 'AIR', 'ASML', 'SAN', 'NVDA', 'META', 'ELF', 'MDB']\n","data_train_X, data_train_y, data_test_X, data_test_y = [], [], [], []\n","for company in list_of_companies:\n","\n","    file = company + '_PERCENTAGES_AND_CLOSE.csv'\n","    df_in = pd.read_csv(file)\n","    df_in['date'] = pd.to_datetime(df_in['date'])\n","    df_in = df_in.sort_values(by='date')\n","    df_in = df_in.drop(columns=['symbol'])\n","\n","    dates = df_in['date']\n","    #dates_train, dates_test = split_data_dates(dates, split)\n","    X=df_in.drop(columns=['target',column_date], axis=1)\n","    #Y será la clase a predecir\n","    y=df_in[column_target]\n","    #dataset= X.values\n","    dataset_y = y.values\n","    df_for_scaled = pd.concat([X,y], axis=1)\n","    dataset = df_for_scaled.values\n","    dataset = dataset.reshape(-1,dataset[0].size)\n","    dataset = dataset.astype(\"float32\")\n","    #dataset_y = dataset_y.reshape(-1,1)\n","    dataset_y = dataset_y.astype(\"float32\")\n","\n","    dataset_X = X.values\n","    #dataset_X = dataset_X.reshape(-1,dataset[0].size)\n","    #dataset_X = dataset_X.astype(\"float32\")\n","    pos = int(round(len(dataset_X) * (1-test_size)))\n","\n","    # scaling datase\n","    scaler = MinMaxScaler(feature_range=(0,1))\n","    dataset = scaler.fit_transform(dataset)\n","    dataset_X = scaler.fit_transform(dataset_X)\n","\n","    for i in range(look_back,pos):\n","        a = dataset_X[i-look_back:i]\n","        data_train_X.append(a)\n","        data_train_y.append(dataset_y[i])\n","    for i in range(pos,len(dataset_X)):\n","        a = dataset_X[i-look_back:i]\n","        data_test_X.append(a)\n","        data_test_y.append(dataset_y[i])\n","\n","X_train, y_train, X_test, y_test = np.array(data_train_X), np.array(data_train_y), np.array(data_test_X), np.array(data_test_y)\n","\n","list_epochs = [20,40,60,100]\n","list_epochs = [60,100] # quitar esta linea queda 60 epocs y 8\n","list_capas = [8]\n","#list_capas = [4,8] # quitar\n","#list_days_back = [10,20,30]\n","#list_days_back = [30] #quitar\n","columnas = ['epochs', 'layers', 'days_back', 'auc_test', 'auc_train', 'acc_test', 'acc_train']\n","# Crear un DataFrame vacío con las columnas\n","df_final = pd.DataFrame(columns=columnas)\n","for epochs in list_epochs:\n","        for layer in list_capas:\n","            try:\n","\n","                model = create_model(X_train, input_layers=layer)\n","                model.compile(optimizer = optimizador, loss = 'mean_squared_error')\n","                hist = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n","                trainPredict = model.predict(X_train)\n","                testPredict = model.predict(X_test)\n","                # reshape\n","                #trainPredict = scaler.inverse_transform(trainPredict)\n","                #trainY = scaler.inverse_transform(y_train)\n","                #testPredict = scaler.inverse_transform(testPredict)\n","                #testY = scaler.inverse_transform(y_test)\n","                testY = y_test\n","                trainY = y_train\n","                auc_test = roc_auc_score(testY, testPredict)\n","                auc_train = roc_auc_score(trainY, trainPredict)\n","                acc_test = accuracy_score(testY, testPredict.round())\n","                acc_train = accuracy_score(trainY, trainPredict.round())\n","\n","                data = {'epochs': [epochs], 'layers': [layer], 'days_back': [look_back],\n","                            'auc_test': [auc_test], 'auc_train': [auc_train], 'acc_test': [acc_test], 'acc_train': [acc_train]}\n","\n","                df_new = pd.DataFrame(data)\n","                df_final = pd.concat([df_final, df_new], axis=0, ignore_index=True)\n","                print(df_final)\n","            except:\n","                pass\n","                print(\"error\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Tr52iu4tSI8"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMYG9szU5wv2IcECX9eFEE3","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}